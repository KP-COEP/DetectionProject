{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kvpcloud/.conda/envs/p3_gpu/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display as ds\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    main = 'dataset/UCSD_Anomaly_Dataset.v1p2'\n",
    "    train_images = np.load('{}/train.npy'.format(main))\n",
    "    train_images_ = np.load('{}/train_.npy'.format(main))\n",
    "    test_images = np.load('{}/test.npy'.format(main))\n",
    "    test_images_ = np.load('{}/test_.npy'.format(main))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 158, 238, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.36078431, 0.36078431, 0.36078431],\n",
       "        [0.35686275, 0.35686275, 0.35686275],\n",
       "        [0.31764706, 0.31764706, 0.31764706],\n",
       "        ...,\n",
       "        [0.16470588, 0.16470588, 0.16470588],\n",
       "        [0.22352941, 0.22352941, 0.22352941],\n",
       "        [0.29019608, 0.29019608, 0.29019608]],\n",
       "\n",
       "       [[0.2745098 , 0.2745098 , 0.2745098 ],\n",
       "        [0.2627451 , 0.2627451 , 0.2627451 ],\n",
       "        [0.27843137, 0.27843137, 0.27843137],\n",
       "        ...,\n",
       "        [0.18823529, 0.18823529, 0.18823529],\n",
       "        [0.19215686, 0.19215686, 0.19215686],\n",
       "        [0.33333333, 0.33333333, 0.33333333]],\n",
       "\n",
       "       [[0.30588235, 0.30588235, 0.30588235],\n",
       "        [0.32941176, 0.32941176, 0.32941176],\n",
       "        [0.3372549 , 0.3372549 , 0.3372549 ],\n",
       "        ...,\n",
       "        [0.24705882, 0.24705882, 0.24705882],\n",
       "        [0.16862745, 0.16862745, 0.16862745],\n",
       "        [0.28235294, 0.28235294, 0.28235294]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.41960784, 0.41960784, 0.41960784],\n",
       "        [0.41960784, 0.41960784, 0.41960784],\n",
       "        [0.43137255, 0.43137255, 0.43137255],\n",
       "        ...,\n",
       "        [0.63921569, 0.63921569, 0.63921569],\n",
       "        [0.62352941, 0.62352941, 0.62352941],\n",
       "        [0.70588235, 0.70588235, 0.70588235]],\n",
       "\n",
       "       [[0.44313725, 0.44313725, 0.44313725],\n",
       "        [0.44313725, 0.44313725, 0.44313725],\n",
       "        [0.44705882, 0.44705882, 0.44705882],\n",
       "        ...,\n",
       "        [0.62745098, 0.62745098, 0.62745098],\n",
       "        [0.63137255, 0.63137255, 0.63137255],\n",
       "        [0.69411765, 0.69411765, 0.69411765]],\n",
       "\n",
       "       [[0.45490196, 0.45490196, 0.45490196],\n",
       "        [0.45490196, 0.45490196, 0.45490196],\n",
       "        [0.45882353, 0.45882353, 0.45882353],\n",
       "        ...,\n",
       "        [0.64313725, 0.64313725, 0.64313725],\n",
       "        [0.63137255, 0.63137255, 0.63137255],\n",
       "        [0.72156863, 0.72156863, 0.72156863]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.train_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    latent_feature_count = [5,5,3]\n",
    "    epochs = 500\n",
    "    reduced_feature_rbf_count = 5 # liklihood fails for 3,4&5 and gets stuck for 2\n",
    "    frames_for_anomaly = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            with tf.name_scope('Input'):\n",
    "                self.x = tf.placeholder(tf.float32, shape=(None, 158, 238, 3), name='X')\n",
    "                self.x_ = tf.placeholder(tf.float32, shape=(None, 158, 238, 3), name='X_')\n",
    "\n",
    "        self.encoder, self.decoder = self.build_network(self.x)\n",
    "\n",
    "    def build_network(self, x):\n",
    "        def encoder(x):\n",
    "            with self.graph.as_default():\n",
    "                with tf.name_scope('Encoder'):\n",
    "                    x = tf.layers.conv2d(x, filters=32, kernel_size=(5,5), strides=(1,1), activation=tf.nn.relu)\n",
    "                    print('After 1st Conv', x.get_shape())\n",
    "                    tf.summary.image('encoder_hidden_1', x[:,:,:,0:3],max_outputs=1)\n",
    "                    \n",
    "                    x = tf.layers.max_pooling2d(x, pool_size=(5,5), strides=(1,1), )\n",
    "                    print('After 1st Pooling', x.get_shape())\n",
    "\n",
    "                    x = tf.layers.conv2d(x, filters=16, kernel_size=(5,5), strides=(1,1), activation=tf.nn.relu)\n",
    "                    print('After 2nd Conv', x.get_shape())\n",
    "                    tf.summary.image('encoder_hidden_2', x[:,:,:,0:3],max_outputs=1)\n",
    "                    x = tf.layers.max_pooling2d(x, pool_size=(5,5), strides=(1,1))\n",
    "                    print('After 2nd Pooling', x.get_shape())\n",
    "\n",
    "                    x = tf.layers.conv2d(x, filters=8, kernel_size=(5,5), strides=(1,1), activation=tf.nn.relu)\n",
    "                    print('After 3rd Conv', x.get_shape())\n",
    "                    tf.summary.image('encoder_hidden_3', x[:,:,:,0:3],max_outputs=1)\n",
    "                    \n",
    "                    encoded = tf.layers.max_pooling2d(x, pool_size=(5,5), strides=(1,1))\n",
    "                    print('After 3rd Pooling (Final Encoded)', x.get_shape())\n",
    "                    tf.summary.image('encoder_hidden_final', encoded[:,:,:,0:3], max_outputs=1)\n",
    "                    \n",
    "                    with tf.name_scope('Latent'):\n",
    "                    \n",
    "                        self.latent = tf.layers.dense(tf.contrib.layers.flatten(encoded), #depricated\n",
    "                                                  units=np.prod(Params.latent_feature_count), \n",
    "                                                  activation=tf.nn.relu)\n",
    "\n",
    "                        print('Latent', self.latent.get_shape())\n",
    "                        tf.summary.histogram('Latent', self.latent)\n",
    "\n",
    "                \n",
    "            return encoded\n",
    "\n",
    "        def decoder(encoded):\n",
    "            with self.graph.as_default():\n",
    "                with tf.name_scope('Decoder'):\n",
    "                    #x = tf.reshape(encoded, [-1] + Params.latent_feature_count)\n",
    "                    x = encoded\n",
    "                    x = tf.layers.conv2d_transpose(x, filters=8, kernel_size=(5,5), strides=(1,1), activation=tf.nn.relu)\n",
    "                    print('After 1st conv transpose', x.get_shape())\n",
    "                    tf.summary.image('decoder_hidden_1', x[:,:,:,0:3],max_outputs=1)\n",
    "                    \n",
    "                    x = tf.layers.conv2d_transpose(x, filters=16, kernel_size=(5,5), strides=(1,1), activation=tf.nn.relu)\n",
    "                    print('After 2nd conv transpose', x.get_shape())\n",
    "                    tf.summary.image('decoder_hidden_2', x[:,:,:,0:3],max_outputs=1)\n",
    "                    \n",
    "                    x = tf.layers.conv2d_transpose(x, filters=32, kernel_size=(5,5), strides=(1,1), activation=tf.nn.relu)\n",
    "                    print('After 3rd conv transpose', x.get_shape())\n",
    "                    tf.summary.image('decoder_hidden_3', x[:,:,:,0:3],max_outputs=1)\n",
    "                    \n",
    "                    x = tf.layers.conv2d_transpose(x, filters=32, kernel_size=(5,5), strides=(1,1), activation=tf.nn.relu)\n",
    "                    print('After 4th conv transpose', x.get_shape())\n",
    "                    tf.summary.image('decoder_hidden_4', x[:,:,:,0:3],max_outputs=1)\n",
    "                    \n",
    "                    x = tf.layers.conv2d_transpose(x, filters=3, kernel_size=(5,5), strides=(1,1), activation=tf.nn.relu)\n",
    "                    print('After 5th conv transpose', x.get_shape())\n",
    "                    tf.summary.image('decoder_hidden_5', x[:,:,:,0:3],max_outputs=1)\n",
    "                    \n",
    "                    decoded = tf.layers.conv2d_transpose(x, filters=3, kernel_size=(5,5), strides=(1,1), activation=tf.nn.sigmoid)\n",
    "                    print('After 6th conv transpose (final decoded)', decoded.get_shape())\n",
    "                    tf.summary.image('decoder_hidden_decoded', decoded[:,:,:,0:3],max_outputs=1)\n",
    "\n",
    "            return decoded\n",
    "        \n",
    "        return encoder, decoder\n",
    "    \n",
    "    def get_encoded(self):\n",
    "        with self.graph.as_default():\n",
    "            encoded = self.encoder(self.x_)\n",
    "\n",
    "        return encoded\n",
    "    \n",
    "    def get_decoded(self):\n",
    "        encoded = self.get_encoded()\n",
    "        x_hat = self.decoder(encoded)\n",
    "        return x_hat\n",
    "    \n",
    "    def lstm(self, latent):\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            with tf.name_scope('LSTM') as lstm_scope:\n",
    "                #x = tf.layers.dense(latent, units=np.prod(Params.latent_feature_count), activation=tf.nn.relu)\n",
    "\n",
    "                x = latent\n",
    "\n",
    "                lstm = tf.contrib.rnn.BasicLSTMCell(64)\n",
    "\n",
    "                # Grouping 10 images into 1 to form a video clip for which Anomaly detection will be done\n",
    "                x_list = tf.split(x, 10, axis=0) \n",
    "\n",
    "                y_list, self.state = tf.nn.static_rnn(lstm, x_list, dtype = tf.float32) \n",
    "\n",
    "                lstm_out = tf.stack(y_list[-1]) # we only need the last output\n",
    "                lstm_out = tf.layers.dense(lstm_out, Params.reduced_feature_rbf_count)\n",
    "                print('Reduced dimensions for RBF', lstm_out.get_shape())\n",
    "                lstm_out = tf.print(lstm_out, [lstm_out], \"********* Output of LSTM, Input to RBF \", summarize = 30)\n",
    "\n",
    "        return lstm_out\n",
    "\n",
    "    def rbf(self, lstm_output):\n",
    "        with self.graph.as_default():\n",
    "            def get_cost(U, Z, Q): \n",
    "\n",
    "                cost = - (-U - tf.log(Z)) \n",
    "                return tf.reduce_mean(cost)\n",
    "\n",
    "            f = Params.reduced_feature_rbf_count #np.prod(Params.latent_feature_count)\n",
    "\n",
    "\n",
    "            with tf.name_scope('RBF'):\n",
    "                X = lstm_output\n",
    "\n",
    "                mu = tf.Variable(tf.random_uniform([1,f], minval=0.1, dtype=tf.float32))\n",
    "                mu = tf.print(mu, [mu], \"************ Mu \")\n",
    "                \n",
    "                #Q_ = tf.Variable(tf.truncated_normal([f], mean = 1)) \n",
    "                global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "                sd = tf.Variable(tf.random_uniform([f], minval=0.1, dtype=tf.float32))\n",
    "\n",
    "                sigma = tf.square(sd)\n",
    "                sigma = tf.print(sigma, [sigma], \"************ Sigma (squared) \")\n",
    "                sigma_inverse = tf.reciprocal(sigma)\n",
    "                sigma_inverse = tf.print(sigma_inverse, [sigma_inverse], '************* sigma_inverse')\n",
    "\n",
    "                cov_inverse = tf.diag(sigma_inverse)\n",
    "\n",
    "                det_sigma = tf.reduce_prod(sigma) #tf.pow(sigma, 0.5))\n",
    "                \n",
    "                z = tf.multiply(2*math.pi, det_sigma)\n",
    "                z = tf.print(z, [z], ' ******* z')\n",
    "\n",
    "                with tf.name_scope('Likelihood'):\n",
    "\n",
    "                    M = X - mu\n",
    "                    \n",
    "                    energy = tf.matmul(tf.matmul(M,cov_inverse), tf.transpose(M)) \n",
    "                    energy = tf.print(energy, [energy], '*********** energy after matmul', summarize=30)\n",
    "    \n",
    "                    energy = tf.matrix_diag_part(energy)\n",
    "                    #print(\"Energy after matmul\", energy.get_shape())\n",
    "                    \n",
    "                    \n",
    "                    #energy = tf.reduce_sum(energy, axis = 1, keepdims = True)\n",
    "                    energy = tf.print(energy, [energy], '*********** Energy after summation', summarize=30)\n",
    "                    print('Energy after summation ', energy.get_shape())\n",
    "\n",
    "                    print('X', X.get_shape())\n",
    "                    print('Covariance', cov_inverse.get_shape())\n",
    "\n",
    "                    expnt = tf.exp(-1 * tf.multiply(energy, 0.5))\n",
    "                    print('exponent', expnt.get_shape())\n",
    "                    expnt = tf.print(expnt, [expnt], \"********* Exponent\")\n",
    "\n",
    "                    Y_ = expnt #tf.nn.sigmoid(expnt) #tf.divide(expnt, z)\n",
    "\n",
    "\n",
    "                    #Y_ = tf.layers.dense(Y_, 1)\n",
    "                    rbf_out = Y_ #tf.reduce_mean(Y_) # tf.layers.dense(Y_, 1)\n",
    "                    Y_ = tf.print(Y_, [Y_], 'XXXXXXXXXXXXXXXXXXXX Liklihood XXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "                    print('Y_', Y_.get_shape())\n",
    "\n",
    "                with tf.name_scope('Loss'):\n",
    "                    #cost = -1 * tf.log(Y_ + 0.0001) # Adding a small delta because log is not a continuous function. \n",
    "                    cost = energy #+ tf.log(z)\n",
    "                    cost = tf.print(cost,[cost], \"******** Cost(always postive)\")\n",
    "                    loss = tf.reduce_mean(cost) #get_cost(U, Z, Q) # 1- Y_[0]\n",
    "                    loss = tf.print(loss, [loss], \"**************** Loss\")\n",
    "                    tf.summary.scalar('loss', loss)\n",
    "\n",
    "        return mu, sd, rbf_out, loss\n",
    "\n",
    "    def get_spatial_loss(self):\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            \n",
    "            self.x_hat = self.get_decoded()\n",
    "            spatial_loss = tf.losses.mean_squared_error(labels=self.x_, predictions=self.x_hat)\n",
    "                \n",
    "        return spatial_loss\n",
    "    \n",
    "    def get_temporal_loss(self):\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            with tf.name_scope('Temporal_Autoencoder'):\n",
    "                \n",
    "                lstm_out = self.lstm(self.latent)\n",
    "                print(\"LSTM: Images will be grouped (e.g. 10 images to 1 clip) into video clips when fed to LSTM\",\n",
    "                      lstm_out.get_shape())\n",
    "                tf.summary.histogram(\"LSTM\", lstm_out)\n",
    "\n",
    "                self.mean, self.sigma, self.likelihood, likelihood_loss = self.rbf(lstm_out)\n",
    "                print(\"Likelihood: \", self.likelihood.get_shape())\n",
    "                tf.summary.scalar(\"Temporal_Loss\", likelihood_loss)\n",
    "                \n",
    "        return likelihood_loss\n",
    "    \n",
    "    \n",
    "    def get_optimizer_loss(self):\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            self.x_hat = self.get_decoded()\n",
    "\n",
    "            with tf.name_scope('Optimization'):\n",
    "                \n",
    "                spatial_loss = self.get_spatial_loss()\n",
    "                temporal_loss = self.get_temporal_loss()\n",
    "                \n",
    "                loss = spatial_loss + temporal_loss\n",
    "                tf.summary.scalar(\"loss\",loss)\n",
    "                \n",
    "                starter_learning_rate = 0.01\n",
    "                global_step = tf.Variable(0, trainable=False)\n",
    "                #global_step = tf.print(global_step, [global_step], \"########## Global Step Completed #############\")\n",
    "                learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                                   int(Params.epochs / 5), 0.5, staircase=True)\n",
    "                train = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "                tf.summary.scalar(\"learning_rate\",learning_rate)\n",
    "\n",
    "            merged = tf.summary.merge_all()\n",
    "        return train, merged, loss, self.likelihood, self.mean, self.sigma\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1st Conv (?, 154, 234, 32)\n",
      "After 1st Pooling (?, 150, 230, 32)\n",
      "After 2nd Conv (?, 146, 226, 16)\n",
      "After 2nd Pooling (?, 142, 222, 16)\n",
      "After 3rd Conv (?, 138, 218, 8)\n",
      "After 3rd Pooling (Final Encoded) (?, 138, 218, 8)\n",
      "Latent (?, 75)\n",
      "After 1st conv transpose (?, 138, 218, 8)\n",
      "After 2nd conv transpose (?, 142, 222, 16)\n",
      "After 3rd conv transpose (?, 146, 226, 32)\n",
      "After 4th conv transpose (?, 150, 230, 32)\n",
      "After 5th conv transpose (?, 154, 234, 3)\n",
      "After 6th conv transpose (final decoded) (?, 158, 238, 3)\n",
      "After 1st Conv (?, 154, 234, 32)\n",
      "After 1st Pooling (?, 150, 230, 32)\n",
      "After 2nd Conv (?, 146, 226, 16)\n",
      "After 2nd Pooling (?, 142, 222, 16)\n",
      "After 3rd Conv (?, 138, 218, 8)\n",
      "After 3rd Pooling (Final Encoded) (?, 138, 218, 8)\n",
      "Latent (?, 75)\n",
      "After 1st conv transpose (?, 138, 218, 8)\n",
      "After 2nd conv transpose (?, 142, 222, 16)\n",
      "After 3rd conv transpose (?, 146, 226, 32)\n",
      "After 4th conv transpose (?, 150, 230, 32)\n",
      "After 5th conv transpose (?, 154, 234, 3)\n",
      "After 6th conv transpose (final decoded) (?, 158, 238, 3)\n",
      "Reduced dimensions for RBF (?, 5)\n",
      "LSTM: Images will be grouped (e.g. 10 images to 1 clip) into video clips when fed to LSTM (?, 5)\n",
      "Energy after summation  (?,)\n",
      "X (?, 5)\n",
      "Covariance (5, 5)\n",
      "exponent (?,)\n",
      "Y_ (?,)\n",
      "Likelihood:  (?,)\n",
      "epoch: 0 loss: 205.28795 Likelihood: [0. 0. 0.] Mean: [0.5368756  0.8112876  0.97822714 0.75512505 0.2393613 ]\n",
      "****validation****\n",
      "Likelihood: [0. 0. 0.] Mean: [0.5368756  0.8112876  0.97822714 0.75512505 0.2393613 ]\n",
      "*******************\n",
      "epoch: 10 loss: 0.3536136 Likelihood: [0.85071856 0.85071856 0.85071856] Mean: [0.5260441 0.7894513 1.0702326 0.7461594 0.2546265]\n",
      "epoch: 20 loss: 0.027970849 Likelihood: [0.99991393 0.99991393 0.99991393] Mean: [0.5263788  0.78927624 1.075203   0.74591833 0.25452605]\n",
      "epoch: 30 loss: 0.027131993 Likelihood: [1. 1. 1.] Mean: [0.5263761  0.7892835  1.0753217  0.74590427 0.25452235]\n",
      "epoch: 40 loss: 0.0268424 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 50 loss: 0.02667514 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 60 loss: 0.02660428 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 70 loss: 0.026553737 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 80 loss: 0.026519915 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 90 loss: 0.026497794 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 100 loss: 0.026482048 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "****validation****\n",
      "Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "*******************\n",
      "epoch: 110 loss: 0.026473 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 120 loss: 0.026465274 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 130 loss: 0.026460635 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 140 loss: 0.026457641 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 150 loss: 0.0264553 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 160 loss: 0.02645404 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 170 loss: 0.026453028 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 180 loss: 0.026452359 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 190 loss: 0.026451923 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 200 loss: 0.02645159 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "****validation****\n",
      "Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "*******************\n",
      "epoch: 210 loss: 0.026451407 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 220 loss: 0.02645125 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 230 loss: 0.026451137 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 240 loss: 0.02645108 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 250 loss: 0.02645103 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 260 loss: 0.026450997 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 270 loss: 0.026450982 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 280 loss: 0.026450962 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 290 loss: 0.026450949 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 300 loss: 0.026450943 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "****validation****\n",
      "Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "*******************\n",
      "epoch: 310 loss: 0.026450945 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 320 loss: 0.026450934 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 330 loss: 0.026450941 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 340 loss: 0.026450938 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 350 loss: 0.026450938 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 360 loss: 0.02645094 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 370 loss: 0.026450938 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 380 loss: 0.026450934 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 390 loss: 0.026450934 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 400 loss: 0.026450938 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "****validation****\n",
      "Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "*******************\n",
      "epoch: 410 loss: 0.026450938 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 420 loss: 0.026450938 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 430 loss: 0.026450938 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n",
      "epoch: 440 loss: 0.026450941 Likelihood: [1. 1. 1.] Mean: [0.5263765  0.7892835  1.0753192  0.7459049  0.25452223]\n"
     ]
    }
   ],
   "source": [
    "loss_arr=[]\n",
    "likelihood_arr=[]\n",
    "valid_likelihood_arr=[]\n",
    "network = Network()\n",
    "\n",
    "m_b_s = 3*Params.frames_for_anomaly\n",
    "with tf.Session(graph=network.graph) as sess:\n",
    "\n",
    "    \n",
    "    optim_loss = network.get_optimizer_loss()\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter('logdir/cae_train', sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(Params.epochs):\n",
    "        m_b_s_old = 0\n",
    "        for m_b_s_new in range(m_b_s, Dataset.train_images_.shape[0], m_b_s  ):\n",
    "            _, merged, loss, likelihood, mean, sigma = sess.run(optim_loss, \n",
    "                                                   feed_dict={network.x:Dataset.train_images_[m_b_s_old:m_b_s_new,...],\n",
    "                                                            network.x_: Dataset.train_images[m_b_s_old:m_b_s_new,...]})\n",
    "\n",
    "            m_b_s_old = m_b_s_new\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print('epoch:', epoch, 'loss:', loss, 'Likelihood:', np.asarray(likelihood).flatten(), 'Mean:', np.asarray(mean).flatten())\n",
    "            train_writer.add_summary(merged, epoch)\n",
    "            loss_arr.append(loss)\n",
    "            likelihood_arr.append(np.mean(likelihood))\n",
    "        if epoch % 100 == 0:\n",
    "            print('****validation****')\n",
    "            valid_likelihood, valid_mean = sess.run([network.likelihood, network.mean],\n",
    "                                                               feed_dict={network.x:Dataset.test_images_[:30,... ],\n",
    "                                                                network.x_: Dataset.test_images[:30,... ]})\n",
    "            \n",
    "            print('Likelihood:', np.asarray(likelihood).flatten(), 'Mean:', np.asarray(mean).flatten())\n",
    "            valid_likelihood_arr.append(np.mean(valid_likelihood))\n",
    "            print('*******************')\n",
    "        \n",
    "    \n",
    "    #Dataset.latent = sess.run(network.latent,feed_dict={network.x:Dataset.train_images_,\n",
    "    #                                        network.x_: Dataset.train_images})\n",
    "    \n",
    "    \n",
    "    print('Getting predictions for Training Data')\n",
    "    Dataset.reproduced_images_training = sess.run(network.x_hat,feed_dict={network.x:Dataset.train_images_[:30,... ],\n",
    "                                            network.x_: Dataset.train_images[:30,... ]})\n",
    "    Dataset.likelihood_training, Dataset.mean_training, Dataset.sigma_training = sess.run([network.likelihood, network.mean, network.sigma],\n",
    "                                                                                          feed_dict={network.x:Dataset.train_images_[:30,... ],\n",
    "                                                                                        network.x_: Dataset.train_images[:30,... ]})\n",
    "    \n",
    "    print('Getting predictions for Testing Data')\n",
    "    Dataset.reproduced_images = sess.run(network.x_hat,feed_dict={network.x:Dataset.test_images_[:30,... ],\n",
    "                                            network.x_: Dataset.test_images[:30,... ]})\n",
    "    Dataset.likelihood, Dataset.mean, Dataset.sigma = sess.run([network.likelihood, network.mean, network.sigma],\n",
    "                                                               feed_dict={network.x:Dataset.test_images_[:30,... ],\n",
    "                                                                network.x_: Dataset.test_images[:30,... ]})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_arr, label='Loss')\n",
    "plt.plot(likelihood_arr, label='Likelihood')\n",
    "plt.plot(valid_likelihood_arr, label='Validation Likelihood')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('{}/latent_train'.format(Dataset.main),Dataset.latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6,5, figsize=(25,8))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, imgs in enumerate(Dataset.test_images_[:30]):\n",
    "    axs[i].imshow(imgs[:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6,5, figsize=(25,8))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, imgs in enumerate(Dataset.reproduced_images[:30]):\n",
    "    axs[i].imshow(imgs[:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.likelihood, Dataset.mean, Dataset.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6,5, figsize=(25,8))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, imgs in enumerate(Dataset.train_images_[:30]):\n",
    "    axs[i].imshow(imgs[:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6,5, figsize=(25,8))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, imgs in enumerate(Dataset.reproduced_images_training[:30]):\n",
    "    axs[i].imshow(imgs[:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.likelihood_training, Dataset.mean_training, Dataset.sigma_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
